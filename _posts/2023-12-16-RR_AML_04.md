---
title: '[Reading record] DeepFool: a simple and accurate method to fool deep neural networks'
date: 2023-12-16
usemathjax: true
permalink: /posts/2023/12/RR_AML_04/
tags:
  - Adversarial Machine learning
  - Reading Record
---
Some records and thinking about "DeepFool: a simple and accurate method to fool deep neural networks"

Abstract
------
Challenge: No effective methods have been proposed to accurately compute the robustness of state-of-the-art deep classifiers to such perturbations on large-scale datasets.

Contribution: Propose the DeepFool algorithm to efficiently compute perturbations that fool deep networks, and thus reliably quantify the robustness of these classifiers.


1.Introduction
------

Define adversarial perturbation as a minimal perturbation $\mathcal{r}$ that is sufficient to change the estimated label $\widehat{k}(\mathcal{x})$:

$
\Delta(\mathcal{x};\widehat{k}):=\mathop{min}\limits_\mathcal{r} \lVert\mathcal{r}\rVert _ 2 \ subject \ to \  \widehat{k}(\mathcal{x}+\mathcal{r}) \neq \widehat{k}(\mathcal{x})
$

where $\mathcal{x}$ is an image and $\widehat{k}(\mathcal{x})$ is the estimated label. We call $\Delta(\mathcal{x};\widehat{k})$ the robustness of $\widehat{k}$ at point $\mathcal{x}$. The robustness of classifier $\widehat{k}$ is defined as:

$ 
\rho_{adv}(\widehat{k}) = \mathbb{E}_x\frac{\Delta(\mathcal{x};\widehat{k})}{ \lVert \mathcal{x} \rVert _ 2}
$

where $\mathbb{E}_x$ is the expectation over the distribution of data.

The state-of-the-art deep network are not robust at all to small adversarial perturbation while adversarial attacks are specific to the classifier(adversarial perturbations are generalizable across different models).


Main contributions:
- propose a simple yet accurate method for computing and comparing the robustness of different classifiers to adversarial perturbations.
- perform an extensive experimental comparison. Show that their method is more reliably and efficiently and augmenting training data with adversarial examples significantly increases the robustness to adversarial perturbation.
- show that using imprecise approaches for the computation of adversarial perturbations could lead to different and sometimes misleading conclusions about the robustness.




2.DeepFool for binary classifiers
------
In this section, the author propose the algorithm for binary classifiers. In this case they begin by analyzing a affine function $\mathit{f}$ where $\mathit{f}(x) = \omega^Tx + b $. So that the robustness of $\mathit{f}$ at point $x_0$ is equal to the distance from $x_0$ to the separating affine hyperplain $\mathcal{F}$.

The minimal perturbation to change the classifier's decision corresponds to the orthogonal projection of $x_0$ onto $\mathcal{F}$

The closed-form formula:

$ r_*(x_0):= arg min \lVert r \rVert _ {2} \ subject\ to \ sign(f(x_0+r)) \neq  sign(f(x_0))$
$ \qquad \ = - \frac{f(x_0)}{\lVert \omega \rVert _{2}^{2} } \omega$

where $ \frac{f(x_0)}{\lVert \omega \rVert _{2} }$ means the distance between the point and hyperplane, and $\frac{\omega}{\lVert \omega \rVert _{2}}$ means the unit vector of the gradient. In reality, NNs are generally highly nonlinear,so in the pseudocode there is a while loop to calculate the step to the boundary of the hyperplane.




3.Gradient descent attacks
------
Similar to binary classification, in multiclass classification, it is necessary to calculate the distance and move in the direction of the hyperplane with the minimum distance.

So the authur define $\widehat{l}(x_0)$ to be the closest hyperplane of the boundry of P, **where P is the convex polyhedron and the problem corresponds to the computation of the distance between $x_0$ and the complement of the convex polyhedron P.**



$
\widehat{l}(x_0) = \mathop{argmin}\limits_{k \neq \widehat{k}(x_0)}\frac{\| f_k(x_0)-f_{\widehat{k}(x_0)}(x_0) \|}{ \lVert \omega _ k - \omega _ {\widehat{k}(x_0)} \rVert _ 2}
$

and the minimum perturbation $r_*(x_0)$ is the vector that project $x_0$ on the hyperplane indexed by $\widehat{l}(x_0)$.

$
r_*(x_0) = \frac{\| f_{\widehat{l}(x_0)}(x_0)-f_{\widehat{k}(x_0)}(x_0) \|}{ \lVert \omega _ {\widehat{l}(x_0)} - \omega _ {\widehat{k}(x_0)} \rVert _ 2 ^2}(\omega_{\widehat{l}(x_0)}-\omega_{\widehat{k}(x_0)})
$












4.Experiment
------

5.Conclusions, limitations and future work
------



[Paper Link](https://openaccess.thecvf.com/content_cvpr_2016/html/Moosavi-Dezfooli_DeepFool_A_Simple_CVPR_2016_paper.html)




Refer:
------  
1.Moosavi-Dezfooli, Seyed-Mohsen, Alhussein Fawzi, and Pascal Frossard. "Deepfool: a simple and accurate method to fool deep neural networks." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.