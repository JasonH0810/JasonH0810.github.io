---
title: '[Reading record] Intriguing properties of neural networks'
date: 2023-09-26
permalink: /posts/2023/09/RR_AML_01/
tags:
  - Adversarial Machine learning
  - Reading Record
---


Some records and thinking about "Intriguing properties of neural networks"

1.Abstract
------
Mainly report two properties of DNN:
- No distinction between individual high level units and random linear combinations of high level units.-> Space contains the semantic info.
- the input-output mappings of DNN are fairly discontinuous.-> Misclassify an image by perturbation. Same perturbation causes different network to misclaasify the same input.

2.Introduction
------
Two counter-intuitive properties of DNN:
> The first property is concerned with the semantic meaning of individual units......Generally, it seems that it is the entire space of activations, rather than the individual units, that contains the bulk of the semantic information.......so the individual units of the vector representations are unlikely to contain semantic information. 


> The second property is concerned with the stability of neural networks with respect to small perturbations to their inputs.......However, we find that applying an imperceptible non-random perturbation to a test image,it is possible to arbitrarily change the networkâ€™s prediction......

Some findings:
> ...we found that adversarial examples are relatively robust......we find that these examples are still statistically hard for another neural network...

DNN learned by BP->nonintuitive characteristics and intrinsic blind spots


3.Framework
------
Some Notation:
> x $\in \mathbb{R}^m$ -> Input image <br/>
>  $\Phi$(x)        ->     activation values of some layer.
> - For MNIST dataset: A simple fully connected network with one or more hidden layers and a Softmax classifier.(FC) A classifier trained on top of an autoencoder.(AE) Using regularization with a weight decay of $\lambda$
> - Image Net:AlexNet
> - Image from Youtube:Unsupervised trained network with 1 Billion learnable parameters.(QuocNet)


4.Units of $\Phi$(x)
------
This section proves the indistinguishableness between the random projection of $\Phi$(x) and the coordinates of $\Phi$(x), and also shows that it's not that individual high level units contain semantic information.


5.Blind spots in Neural Network
------
Unit-level inspection methods are not enough to explain the complexity of the representations learned by a deep neural network, so the author use a global network level inspection to explain classification decisions.

> ...In other words, it is assumed that is possible for the output unit to assign nonsignificant (and, presumably, non-epsilon) probabilities to regions of the input space that contain no training examples in their vicinity...for a small enough radius $\epsilon>0$ in the vicinity of a given training input x, an x + r satisfying &#124;&#124;r&#124;&#124; < $\epsilon$ will get assigned a high probability of the correct class by the model. This kind of smoothness prior is typically valid for computer vision problems......**for deep neural networks, the smoothness assumption that underlies many kernel methods does not hold**...

How to find adversarial example?
------

The author follows the idea of hard-negative mining, and proposes a new method to generate adversarial samples by exploiting flaws in modeling the local space around the model and training data.

$$Minimize{\,}c|r| + loss_f(x+r,l) {\,}subject{\,} to {\,}x+r \in [0,1]^m $$


6.result and conclusion
------
> ...adversarial examples are somewhat universal and not just the results of overfitting to a particular model or to the specific selection of the training set......back-feeding adversarial examples to training might improve generalization of the resulting models...

> ...but our first qualitative experiments with AlexNet gives us reason to believe that convolutional networks may behave similarly as well...

> ...the adversarial examples show that there exist small additive perturbations of the input (in Euclidean sense) that produce large perturbations at the output of the last layer...

> ...The general conclusion is that adversarial examples tend to stay hard even for models trained with different hyperparameters...

>Still, this experiment leaves open the question of dependence over the training set. Does the hardness of the generated examples rely solely on the particular choice of our training set as a sample or does this effect generalize even to models trained on completely different training sets?

>...The intriguing conclusion is that the adversarial examples remain hard for models trained even on a disjoint training set, although their effectiveness decreases considerably...




This article discusses two counterintuitive features of DNN, starting with an examination from the perspective of individual units. It demonstrates the phenomenon that "random projections of $\Phi(x)$ are semantically indistinguishable from the coordinates of $\Phi(x)$," indicating that the space may contain more semantic information than individual units. Then, the article conducts an inspection at a global level and discovers the existence of "blind spots" in neural networks. It proposes an adversarial example generation method based on box-constrained L-BFGS. This article can be considered a pioneering work in the field of adversarial machine learning.


[Paper Link](https://arxiv.org/abs/1312.6199)

[Reproduction](https://github.com/JasonH0810/AML_reproduce/tree/main/Intriguing%20properties%20of%20nerural%20networks)

Code
------
    for num in range(10) :
    
      r = torch.rand(1, 28, 28) #Randomly generate a vector r
      r.requires_grad_() 

      optimizer_adv = optim.Adam([r], lr=0.001)

      for i in range(3000):

          X = torch.clamp(sample_img + r, 0, 1)  #make sure that the value of sample_img is between 0-1 after adding r
          Y = torch.tensor([num]) #"num" is the label that is expected to be finally recognized by the model.

          outputs, _, _ = model(X)
          _, predicted = torch.max(outputs.data, 1) 


          loss_adv = r.abs().sum() + loss(outputs, Y) #loss_func

          optimizer_adv.zero_grad()
          loss_adv.backward(retain_graph=True)
          optimizer_adv.step()

    if predicted.item() != 7 :
        print("Attack successed!")
        imshow(torchvision.utils.make_grid(X.data.cpu(), normalize=True), "Predicted label : " + str(predicted.item()))
        
        continue

The core idea: After the image receives the perturbation, the label changes to the target label and the value of the perturbation r should be as small as possible.

Refer:
------  
1.Szegedy, Christian, et al. "Intriguing properties of neural networks." arXiv preprint arXiv:1312.6199 (2013).