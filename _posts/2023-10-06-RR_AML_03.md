---
title: '[Reading record] Evasion attacks against machine learning at test time'
date: 2023-10-06
permalink: /posts/2023/10/RR_AML_03/
tags:
  - Adversarial Machine learning
  - Reading Record
---
Some records and thinking about "Evasion attacks against machine learning at test time"

Abstract
------
> ...present a simple but effective gradientbased approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks......we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker’s knowledge of the system and her ability to manipulate attack samples...



1.Introduction
------
background: ML is being increasely used in security-sensitive applications

Question/Problem: 
- Due to the assumption of classical ML that the underlying data distribution is stationary and the arms race between designers and adversaries, classical performance evaluation techniques are not suitable to reliably assess the security of learning algorithms.
- Two previous approach:The min-max approach and game-theoretic approach provide a secure counterpart to their respective learning problems, but the realistic constraints are more complex and multi-faceted.
- Limited to linear and convex-inducing classifiers in prior work.


Contribution:
- investigate the vulnerabilities of classification algorithms by deriving evasion attacks.
- systematically assess classifier security in attack scenarios that exhibit increasing risk levels, simulated by increasing the attacker’s knowledge of the system and her ability to manipulate attack samples.
- allows a classifier designer to understand how the classification performance of each considered model degrades under attack and to make more informed design choices.
- propose a novel method based on gradient-descent approach inspired by Golland’s discriminative directions technique.


2.Optimal evasion at test time
------
Problem definition:

A classification algorithm $f = \mathcal{X} \to \mathcal{Y}$ that assigns samples $x \in \mathcal{X}$ to a label $y \in \mathcal{Y} = {-1,+1}$ , where -1(+1) represents legitimate (malicious) class. $f$ is trained on a dataset $\mathcal{D}={x_i,y_i}_{i=1}^n$ sampled from an underlying distribution $p(\mathcal{X} , Y)$ ,The label $y^c = f (x)$ given by a classifier is typically obtained by thresholding a continuous discriminant function $g :  \mathcal{X} \to \mathbb{R}$ .

Adversary model

- Adversary's goal
  - should be defined in terms of a utility (loss) function that the adversary seeks to maximize (minimize); to manipulate a single (without loss of generality, positive) sample that should be misclassified in the evasion setting. But are easily thwarted by slightly adjusting the decision threshold.
  - A better strategy:create a sample that is misclassified with high confidence

- Adversary’s knowledge
  - the training set or part of it
  - the feature representation of each sample
  - the type of a learning algorithm and the form of its decision function
  - the (trained) classifier model
  - feedback from the classifier


- Adversary’s capability:In the evasion scenario, the adversary’s capability is limited to modifications of test data. Under this restriction, variations in attacker’s power may include:
  - modifications to the input data (limited or unlimited)
  - modifications to the input data (limited or unlimited)
  - independent modifications to specific features
  

Attack scenarios:consider two attack scenarios characterized by different levels of adversary’s knowledge of the attacked system

- Perfect knowledge(PK):
  - the adversary’s goal is to minimize $g(x)$
  - has perfect knowledge of the targeted classifier(knows the feature space; the type of the classifier; and the trained model etc.)
  - can transform attack points in the test data but must remain within a maximum distance of $d_{max}$ from the original attack sample.

- Limited knowledge (LK):
  - the adversary aims to minimize the discriminant function $g(x)$ under the same constraint that each transformed attack point must remain within a maximum distance of dmax from the corresponding original attack sample.
  - the attacker knows the feature representation and the type of the classifier, but does not know either the learned classifier $f$ or its training data.
  - can collect a surrogate dataset of samples drawn from the same underlying distribution from which the original dataset was drawn.


Attack strategy

> an optimal attack strategy finds a sample $x^*$ to minimize $g(\centerdot)$ or its estimate $ \widehat{g}(\centerdot)$, subject to a bound on its distance from $x^0$:



$
x^*=arg\mathop{min}_{x}\widehat{g}(x)\quad s.t.\quad d(x,x^0) \leq d _ {max}
$



> ...One may approach it with many well-known techniques, like gradient descent, or quadratic techniques such as Newton’s method, BFGS, or L-BFGS. We choose a gradient-descent procedure...

shortcoming: ...$\widehat{g}(x)$ may be non-convex and descent approaches may not achieve a global optima......Locally optimizing $ \widehat{g}(x)$ with gradient descent is particularly susceptible to failure due to the nature of a discriminant function......when our gradient descent procedure produces an evasion example in these regions, the attacker cannot be confident that this sample will actually evade the corresponding classifier......

overcome: introduce an additional component into our attack objective, which estimates $p(x \mid y^c = −1)$ using a density estimator.

$
arg \mathop{min}_{x} F(x) = \widehat{g}(x) - \frac{\lambda}{n} \sum\limits _ {i \mid y_i^c = -1}  k(\frac{x-x_i}{h}) \quad  s.t. \quad d(x,x^0) \le d _ {max}
$



3.Gradient descent attacks
------
Gradients of discriminant functions
- Linear classifiers: $g(x)= \langle w,x \rangle + b$  where gradient is $\bigtriangledown g(x) = w$
- Support vector machines: $g(x) = \sum_i \alpha_i y_i k(x,x_i) + b$ where gradient is $\bigtriangledown g(x) = \sum_i \alpha_i y_i\bigtriangledown k(x,x_i)$
- Neural networks: $ g(x) = (1+e^{-h(x)})^{-1} $ where $h(x) = \sum^m_{k=1} \omega_k \delta_k(x) + b$ , $ \delta_k(x) = (1+e^{-h_k(x)})^{-1}$ , $h_k(x)=\sum^d_{j=1} \upsilon_{kj}\chi_j + b_k$


Gradients of kernel density estimators

- generalized RBF kernels -> $k(\frac{x-x_i}{h}) = exp(-\frac{d(x,x_i)}{h})$ where $d(\dot, \dot)$ is any suitable distance function.


Descent in discrete spaces

> In discrete spaces, gradient approaches travel through infeasible portions of the feature space......A simple approach to this problem is to probe F at every point in a small neighborhood of x, which would however require a large number of queries......we can instead select the neighbor whose change best aligns with $\bigtriangledown F(x)$ and decreases the objective function



4.Experiment
------

Two experiments: one is based on MNIST dataset and the other is based on the detection of malware PDF file

5.Conclusions, limitations and future work
------

> ...proposed a simple algorithm for evasion of classifiers with differentiable discriminant functions......very popular classification algorithms (in particular, SVMs and neural networks) can still be evaded with high probability even if the adversary can only learn a copy of the classifier from a small surrogate dataset......We believe that the proposed attack formulation can be extended to classifiers with non-differentiable discriminant functions as well......more secure classifiers can be designed by employing regularization terms that promote enclosure of the legitimate class......one may explicitly model the attack distribution......or add the generated attack samples to the training set...

> ...Another idea would be to modify the real-world object at each step of the gradient descent to obtain a sample in the feature space which is as close as possible to the sample that would be obtained at the next attack iteration...

> Other interesting extensions of our work may be to
  - consider more effective strategies to build a small but representative set of surrogate data
  - improve the classifier estimate



[Paper Link](https://arxiv.org/abs/1708.06131)




Refer:
------  
1.Biggio, Battista, et al. "Evasion attacks against machine learning at test time." Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, Part III 13. Springer Berlin Heidelberg, 2013.