---
title: '[Reading record] Explaining and harnessing adversarial example'
date: 2023-10-01
permalink: /posts/2023/10/RR_AML_02/
tags:
  - Adversarial Machine learning
  - Reading Record
---
Some records and thinking about "Explaining And Harnessing Adversarial Example"

1.Abstract
------

- NN's linear nature causes to the vulnerability to adversarial perturbation.

- "NN's generalization across architectures and training sets" yields a simple and fast method of generating adversarial example.

2.Introduction
------
> ...In many cases, a wide variety of models with different architectures trained on different subsets of the training data misclassify the same adversarial example......Linear behavior in high-dimensional spaces is sufficient to cause adversarial examples. This view enables us to design a fast method of generating adversarial examples that makes adversarial training practical...adversarial training can provide an additional regularization benefit beyond that provided by using dropout alone.


> Our explanation suggests a fundamental tension between designing models that are easy to train due to their linearity and designing models that use nonlinear effects to resist adversarial perturbation. In the long run, it may be possible to escape this tradeoff by designing more powerful optimization methods that can succesfully train more nonlinear models.

3.Related work
------
> ...classifiers based on modern machine learning techniques......are not learning the true underlying concepts that determine the correct output label...

> ...use convolutional network features as a space......This resemblance is clearly flawed if images that have an immeasurably small perceptual distance correspond to completely different classes in the network’s representation...


4.The linear explanation of adversarial examples
------
The precision of input feature is limited -> If the perturbation is small enough, it is not rational that the classifier responds differently to x and x+ $\eta$ 


But the adversarial perturbation will cause the activation to grow by $w^T \eta$ . So for the high dimensional problems, many infinitesimal changes to the input can add up to one large change to the output, which shows that a simple linear model can have adversarial examples if its input has sufficient dimensionality.


5.Linear perturbation of non-linear models
------
> ...We hypothesize that neural networks are too linear to resist linear adversarial perturbation......are all intentionally designed to behave in very linear ways, so that they are easier to optimize. More nonlinear models such as sigmoid networks are carefully tuned to spend most of their time in the non-saturating, more linear regime for the same reason...

The authors propose a "fast gradient sign method" of generating adversarial examples.

>$\eta = \epsilon sign(\bigtriangledown_x J(\theta,x,y))$


6.Adversarial traning of linear models versus weight decay
------
> Smaller weight decay coefficients permitted succesful training but conferred no regularization benefit.

7.Adversarial traning of deep network
------
> ...Shallow linear models are not able to become constant near training points while also assigning different outputs to different training points......training with an adversarial objective function based on the fast gradient sign method was an effective regularizer......This approach means that we continually update our supply of adversarial examples, to make them resist the current version of the model.

> We observed that we were not reaching zero error rate on adversarial examples on the training set. We fixed this problem by making two changes.
  - First, we made the model larger, using 1600 units per layer rather than the 240 used by the original maxout network for this problem.
  - ...The original maxout result uses early stopping, and terminates learning after the validation set error rate has not decreased for 100 epochs......We therefore used early stopping on the adversarial validation set error...

> ...Adversarial training can also be seen as a form of active learning, where the model is able to request labels on new points......We could also regularize the model to be insensitive to changes in its features that are smaller than the $\epsilon$ precision simply by training on all points within the $\epsilon$ max norm box, or sampling many points within this box...

> Q:whether it is better to perturb the input or the hidden layers or both.
  - Szegedy et al. reported that adversarial perturbations yield the best regularization when applied to the hidden layers. That result was obtained on a sigmoidal network.
  - In auther's experiment, they find that networks with hidden units whose activations are unbounded simply respond by making their hidden unit activations very large, so it is usually better to just perturb the original input.
  - Authers' view of adversarial training is that it is only clearly useful when the model has the capacity to learn to resist adversarial examples......best results with training using perturbations of hidden layers never involved perturbations of the final hidden layer.


8.Different kinds of model capacity
------

> One reason that the existence of adversarial examples can seem counter-intuitive is that most of us have poor intuitions for high dimensional spaces......Many people think of models with low capacity as being unable to make many different confident predictions,this is not correct.......


Take RBF as example:
> RBF networks are naturally immune to adversarial examples, in the sense that they have low confidence when they are fooled......We can’t expect a model with such low capacity to get the right answer at all points of space, but it does correctly respond by reducing its confidence considerably on points it does not “understand.”......RBF units are unfortunately not invariant to any significant transformations so they cannot generalize very well......RBF units achieve high precision by responding only to a specific point in space, but in doing so sacrifice recall...


9.Why do adversarial examples generalize?
------
> ...an example generated for one model is often misclassified by other models......when these different models misclassify an adversarial example, they often agree with each other on its class...

> To explain why mutiple classifiers assign the same class to adversarial examples, we hypothesize that neural networks trained with current methodologies all resemble the linear classifier learned on the same training set. This reference classifier is able to learn approximately the same classification weights when trained on different subsets of the training set, simply because machine learning algorithms are able to generalize. The stability of the underlying classification weights in turn results in the stability of adversarial examples.

> Our hypothesis does not explain all of the maxout network’s mistakes or all of the mistakes that generalize across models, but clearly a significant proportion of them are consistent with linear behavior being a major cause of cross-model generalization.

10.alternative hypotheses
------
Consider and refute some alternative hypotheses for the existence of adversarial examples.
> 
  - one hypothesis is that generative training could provide more constraint on the training process, or cause the model to learn what to distinguish “real” from “fake” data and be confident only on “real” data......It remains possible that some other form of generative training could confer resistance, but clearly the mere fact of being generative is not alone sufficient.
  - Another hypothesis about why adversarial examples exist is that individual models have strange quirks but averaging over many models can cause adversarial examples to wash out......Ensembling provides only limited resistance to adversarial perturbation.

11.Summary and discussion

> As a summary, this paper has made the following observations:
  - Adversarial examples can be explained as a property of high-dimensional dot products. They are a result of models being too linear, rather than too nonlinear. 
  - The generalization of adversarial examples across different models can be explained as a result of adversarial perturbations being highly aligned with the weight vectors of a model, and different models learning similar functions when trained to perform the same task.
  - The direction of perturbation, rather than the specific point in space, matters most. Space is not full of pockets of adversarial examples that finely tile the reals like the rational numbers. 
  - Because it is the direction that matters most, adversarial perturbations generalize across different clean examples.
  - We have introduced a family of fast methods for generating adversarial examples.
  - We have demonstrated that adversarial training can result in regularization; even further regularization than dropout.
  - We have run control experiments that failed to reproduce this effect with simpler but less efficient regularizers including L1 weight decay and adding noise.
  - Models that are easy to optimize are easy to perturb.
  - Linear models lack the capacity to resist adversarial perturbation; only structures with a hidden layer (where the universal approximator theorem applies) should be trained to resist adversarial perturbation.
  - RBF networks are resistant to adversarial examples.
  - Models trained to model the input distribution are not resistant to adversarial examples.
  - Ensembles are not resistant to adversarial examples.


[Paper Link](https://arxiv.org/pdf/1412.6572.pdf)

[Reproduction](https://github.com/JasonH0810/AML_reproduce/tree/main/EXPLAINING%20AND%20HARNESSING%20ADVERSARIAL%20EXAMPLES)


Code
------
    def fgsm_attack(model, loss, images, labels, eps) :
    
      images = images.to(device)
      labels = labels.to(device)
      images.requires_grad = True
            
      outputs = model(images)
    
      model.zero_grad()
      cost = loss(outputs, labels).to(device) 
      cost.backward()
    
      attack_images = images + eps*images.grad.sign() 
      attack_images = torch.clamp(attack_images, 0, 1)
    
      return attack_images


dateset:ImageNet

feature: 3*299*299 class:1000

target model:Inception_v3

In general, machine learning models compute gradients and update their weights in the direction of gradient descent. The FGSM(fast gradient sign method) attack accomplishes model evasion by introducing numerous small perturbations into the input image that align with the direction of gradient increase. This manipulation causes the image to remain perceptually similar while misleading the model, consequently resulting in misclassification.

FGSM is a fast and easy method to generate adversarial example. but comparing to "box-constrained L-BFGS", it cannot specify which incorrect classes the model should classify the adversarial samples into.

12.16 update:FGSM (Fast Gradient Sign Method) is an approach that can rapidly generate adversarial samples. It only requires a few attempts to determine the value of epsilon (eps) in order to obtain an adversarial sample that closely resembles the original sample. However, this is limited to making the model's predictions for adversarial samples different from those for the original samples. When it comes to specifying the predicted label for an adversarial sample, FGSM's accuracy is relatively low, making it challenging to find an epsilon value that results in the predicted label being the specified one. The underlying reasons for this may be attributed to issues related to the distribution of the sample space and characteristics of the training set. To delve deeper into the specifics, further reading of research papers and learning is required.

Refer:
------  
1.Szegedy, Christian, et al. "Intriguing properties of neural networks." arXiv preprint arXiv:1312.6199 (2013).

2.Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. "Explaining and harnessing adversarial examples." arXiv preprint arXiv:1412.6572 (2014).